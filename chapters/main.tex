% vi:ft=tex

\section{Problem statement}

\paragraph{}
For both \glssymbol{pbs} and \gls{spark}, their own daemons for the cluster are
required to be running on the nodes in the cluster and should have information
about the node like number of CPU cores, memory available for use etc. Since
they are two completely different programs, they have no way of knowing which
node is in use by the other program at a given time and thus create problems
when scheduling.

\paragraph{}
The current workaround for the said problem is to segregate the nodes cluster
into two partitions --- \glssymbol{hpc} cluster and \gls{spark} cluster. But
this creates another problem. At any given time if the demand for running
\glssymbol{hpc} jobs is high and the demand for \gls{spark} jobs is low, the
\gls{spark} cluster will remain idle while \glssymbol{hpc} jobs may end up in
queued state. This is a gross underutilization of the cluster for which the
organization is paying but is unable to efficiently use.


\section{Potential solutions}

The potential solutions to this problem can be:
\begin{enumerate}
    \item Running a \gls{executor} on the \glssymbol{pbs} cluster
        endlessly. This can be done without modifying any code anywhere but it
        increases the need for human interaction. \glssymbol{pbs} jobs also
        have a \gls{walltime} for all jobs. Running a \gls{executor}
        endlessly also violates that condition.
    \item Starting \glspl{executor} as \glssymbol{pbs} jobs on demand.
        This option requires knowing when a \gls{spark} job is submitted and how
        must resources are required for it.
\end{enumerate}


\section{Solution description}

\paragraph{}
The solution which seems most feasible is to start \glspl{executor} on
demand as \glssymbol{hpc} jobs. To do this, we must know how many executors or
memory or CPU cores does the spark application require. This information can
only be acquired if we hack into the \gls{spark} codebase.



\section{Implementation}

\subsection{Submitting \gls{spark} applications on \glssymbol{pbs} cluster}

\paragraph{} There is already support in \gls{spark} code-base for \gls{yarn},
\gls{mesos} and \gls{kubernetes}. This is done by overriding the following
classes in Spark:

\paragraph{ExternalClusterManager} This class is responsible to create an
instance of \texttt{SchedulerBackend} and \texttt{TaskScheduler}, and register
them with the \gls{spark} application. It is discovered by the Spark build by
the help of the \texttt{org.apache.spark.scheduler.ExternalClusterManager} file
in \sloppy{\texttt{resource-managers/pbs/src/main/resources/META-INF/services/}}
directory.

\paragraph{SchedulerBackend} It is the back-end for the \gls{driver},
responsible for starting the \glspl{executor} in the \gls{spark} cluster. We
override the \texttt{SchedulerBackend} so that instead of starting
\glspl{executor} on a \gls{spark} Slave, it now submits the \glspl{executor} as
jobs to \glssymbol{pbs} via \texttt{qsub}.

\paragraph{TaskScheduler} It schedules the sub-tasks for each application and
sends them to the \glspl{executor} registered to that application. We chose not
to override this and keep using the \gls{spark}'s implementation because we need
not start a new \gls{executor} for each task but keep using the already started
\gls{executor}. This is the Coarse Grained mode.

\paragraph{SparkApplication}
The above setup works well for a job where the \gls{driver} is run locally. But
to allow the \gls{driver} to run remotely on a cluster, we must also be able to
submit the \gls{driver} as a job in the cluster. To do this, we also override
\texttt{SparkApplication} which is instantiated whenever a \gls{spark}
application is submitted, to start the \gls{driver} as a \glssymbol{pbs} job in
the cluster.

\begin{figure}[h]
    \centering
    \fbox{
        \includegraphics[width=0.75 \textwidth, natwidth=1280, natheight=900]
            {assets/images/Cluster.png}
        }
    \caption{Spark Cluster UI page displaying jobs on PBS Cluster}
\end{figure}


\subsection{Listing running \gls{spark} applications}

\paragraph{\gls{spark} applications in qstat} The \gls{spark}
application driver running on the \glssymbol{pbs} cluster shows up in the
\texttt{qstat} as ``\texttt{sparkjob-}'' followed by the class name of the
application. The executors connected to this application show up as
``\texttt{sparkexec-}'' followed by the application name. The application
driver does not show up when running in client mode which is expected.

\paragraph{\gls{spark} applications in Web UI} The Spark applications running on
the cluster get their own driver web UIs which are accessed on the \texttt{4040}
port. There is also a master web UI page which is created by running the
\texttt{org.apache.spark.deploy.pbs.ui.PbsClusterUI} class. This starts the
cluster UI which lists all the \gls{spark} applications on the cluster. The user
can also kill the applications by clicking the ``kill'' link on this page.

\begin{figure}[h]
    \centering
    \fbox{
        \includegraphics[width=0.75 \textwidth, natwidth=1280, natheight=849]
            {assets/images/Jobs.png}
        }
    \caption{Spark Job page for each Spark Driver}
\end{figure}


\subsection{Viewing results of the \gls{spark} application}

\paragraph{} The results of the completed \gls{spark} application are stored as
\glssymbol{pbs} job logs. The \texttt{stdout} and \texttt{stderr} are stored in
separate files just like normal a \glssymbol{pbs} job. This can be improved
later by making Web UI to read and display these logs to the user as a web page.


\newpage
\section{Result}

\subsection{Installation}
\paragraph{}
The \gls{spark} application has to be patched to allow setting \glssymbol{pbs}
as a scheduler. Apart from this, the \gls{spark} application must be compiled
with the overridden classes. This whole compiled version of \gls{spark} must be
installed on each execution node of the cluster as well as the client node.
In addtion, to submit an application to \gls{spark} on \glssymbol{pbs} cluster,
there must also be a \glssymbol{pbs} client distribution installed on the client
machine.

\begin{figure}[h]
    \centering
    \fbox{
        \includegraphics[width=0.75 \textwidth, natwidth=1280, natheight=849]
            {assets/images/Executors.png}
        }
    \caption{Job page listing executors which are running on PBS cluster nodes}
\end{figure}


\subsection{Usage}
\paragraph{}
The \gls{spark} application can be submitted just like it used to be submitted
before the \glssymbol{pbs} integration with the commands following the similar
pattern:

\begin{samepage}
    \begin{itemize}
        \item Submit \gls{spark} application on \glssymbol{pbs} cluster:\\
            \texttt{bin/spark-shell --master pbs}

        \item Submit application with \gls{driver} running locally:\\
            \texttt{bin/spark-submit --master pbs --deploy-mode client}

        \item Submit application with \gls{driver} running on the cluster:\\
            \texttt{bin/spark-submit --master pbs --deploy-mode cluster}
    \end{itemize}
\end{samepage}


\begin{figure}[h]
    \centering
    \fbox{
        \includegraphics[width=0.75 \textwidth, natwidth=1280, natheight=897]
            {assets/images/App.png}
        }
    \caption{Spark Application page displaying PBS Job properties}
\end{figure}

\paragraph{}
In this prototype, the result for the \gls{spark} application has to be checked
using the PBS job logs as of now but that can be improved once the UI is in
place. The master UI can read the logs of the job and display them on the
application page in the webUI.
